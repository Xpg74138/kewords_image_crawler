import os
import csv
import time
import json
import hashlib
import random
import urllib.parse
from urllib.parse import urlparse

import requests
from bs4 import BeautifulSoup


# --------------------------
# é…ç½®åŒºï¼ˆå¯ä¿®æ”¹ï¼‰
# --------------------------
CONFIG_PATH = "config.txt"
OUTPUT_ROOT = "images"
METADATA_FILE = "metadata.csv"
MAX_PER_KEYWORD = 5          # æ¯ä¸ªå…³é”®è¯æœ€å¤šä¸‹è½½å¤šå°‘å¼ 
REQUEST_DELAY_RANGE = (1, 3)   # æ¯æ¬¡åˆ†é¡µæŠ“å–åçš„éšæœºå»¶è¿Ÿï¼ˆç§’ï¼‰ä»¥é™ä½è¢«å°é£é™©
TIMEOUT = 10                   # ä¸‹è½½è¶…æ—¶ï¼ˆç§’ï¼‰
HEADERS = {
    "User-Agent":
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
        "AppleWebKit/537.36 (KHTML, like Gecko) "
        "Chrome/123.0.0.0 Safari/537.36"
}


# --------------------------
# å·¥å…·å‡½æ•°
# --------------------------
def read_keywords(path):
    with open(path, "r", encoding="utf-8") as f:
        return [line.strip() for line in f if line.strip()]


def md5_file(path):
    h = hashlib.md5()
    with open(path, "rb") as f:
        h.update(f.read())
    return h.hexdigest()


def safe_filename(s, idx=None, ext="jpg"):
    # å°†ä¸å¯ç”¨å­—ç¬¦æ›¿æ¢ä¸ºä¸‹åˆ’çº¿
    base = "".join(c if c.isalnum() or c in "_-" else "_" for c in s)
    if idx is not None:
        base = f"{base}_{idx:04d}"
    return f"{base}.{ext}"


def guess_ext_from_url(url):
    # ä¾æ® URL åç¼€ç²—ç•¥çŒœæµ‹ï¼›é»˜è®¤ jpg
    path = urlparse(url).path
    _, _, tail = path.rpartition(".")
    tail = tail.lower()
    if tail in ("jpg", "jpeg", "png", "gif", "bmp", "webp"):
        return "jpg" if tail == "jpeg" else tail
    return "jpg"


# --------------------------
# Bing HTML æŠ“å– & è§£æ
# --------------------------
def bing_image_search_html(query, first=0):
    """
    è¿”å› Bing å›¾ç‰‡æœç´¢é¡µé¢ HTML æ–‡æœ¬ã€‚
    first: åˆ†é¡µåç§»ï¼ˆ0, 35, 70...ï¼‰
    """
    params = {
        "q": query,
        "first": str(first),  # èµ·å§‹ç´¢å¼•
        "form": "HDRSC2",
        "cw": "1116", "ch": "777"  # ç»™å®šçª—å£å°ºå¯¸å‚æ•°ï¼ˆå¯é€‰ï¼‰
    }
    url = "https://www.bing.com/images/search?" + urllib.parse.urlencode(params, safe=":+")
    resp = requests.get(url, headers=HEADERS, timeout=TIMEOUT)
    resp.raise_for_status()
    return resp.text


def parse_bing_results(html):
    """
    ä» Bing å›¾ç‰‡ HTML ä¸­æå–ç»“æœåˆ—è¡¨ã€‚
    è¿”å›åˆ—è¡¨ï¼š[{murl:..., purl:..., turl:..., title:...}, ...]
    """
    soup = BeautifulSoup(html, "html.parser")
    results = []
    # æ¯ä¸ªç»“æœåœ¨ <a class="iusc"> æ ‡ç­¾ä¸Š
    for a in soup.find_all("a", class_="iusc"):
        m_attr = a.get("m")
        if not m_attr:
            continue
        # è¯¥å±æ€§æ˜¯ JSON å­—ç¬¦ä¸²ï¼ˆæœ‰æ—¶å« HTML è½¬ä¹‰ &quot;ï¼‰
        try:
            m_json = json.loads(m_attr)
        except json.JSONDecodeError:
            # å°è¯•æ›¿æ¢ HTML å®ä½“å†è§£æ
            try:
                m_json = json.loads(m_attr.replace("&quot;", '"'))
            except Exception:
                continue

        murl = m_json.get("murl")   # å›¾ç‰‡ç›´é“¾
        purl = m_json.get("purl")   # å›¾ç‰‡æ‰€åœ¨ç½‘é¡µ âœ…
        turl = m_json.get("turl")   # ç¼©ç•¥å›¾
        title = m_json.get("t")     # æ ‡é¢˜
        if not murl:
            continue
        results.append({
            "murl": murl,
            "purl": purl,
            "turl": turl,
            "title": title
        })
    return results


# --------------------------
# ä¸‹è½½å•å¼ å›¾ç‰‡
# --------------------------
def download_image(url, save_path):
    try:
        r = requests.get(url, headers=HEADERS, timeout=TIMEOUT, stream=True)
        r.raise_for_status()
        with open(save_path, "wb") as f:
            for chunk in r.iter_content(chunk_size=8192):
                if chunk:
                    f.write(chunk)
        return True
    except Exception as e:
        print(f"   âœ˜ ä¸‹è½½å¤±è´¥: {url} | {e}")
        return False


# --------------------------
# ä¸»æŠ“å–æµç¨‹ï¼ˆå•å…³é”®è¯ï¼‰
# --------------------------
def crawl_one_keyword(keyword, max_num, csv_writer, global_hash_set):
    print(f"\nğŸš€ å¼€å§‹å…³é”®è¯ï¼š{keyword}")
    save_dir = os.path.join(OUTPUT_ROOT, keyword.replace(" ", "_"))
    os.makedirs(save_dir, exist_ok=True)

    downloaded = 0
    page_first = 0
    seen_purl = set()  # æ¯å…³é”®è¯å†…é¿å…åŒç½‘é¡µé‡å¤å†™å¤ªå¤šï¼ˆå¯é€‰ï¼‰

    while downloaded < max_num:
        # æŠ“ä¸€é¡µ HTML
        html = bing_image_search_html(keyword, first=page_first)
        items = parse_bing_results(html)
        if not items:
            print("   âš ï¸ æ²¡æœ‰æ›´å¤šç»“æœï¼Œæå‰ç»“æŸã€‚")
            break

        for item in items:
            if downloaded >= max_num:
                break

            img_url = item["murl"]
            page_url = item.get("purl") or ""
            title = item.get("title") or keyword

            # ä¸‹è½½æ–‡ä»¶
            ext = guess_ext_from_url(img_url)
            filename = safe_filename(keyword, idx=downloaded + 1, ext=ext)
            local_path = os.path.join(save_dir, filename)

            ok = download_image(img_url, local_path)
            if not ok:
                continue

            # å»é‡ï¼ˆè·¨å…³é”®è¯å…¨å±€ï¼‰
            try:
                h = md5_file(local_path)
            except Exception:
                print("   âš ï¸ æ— æ³•è¯»æ–‡ä»¶åšå“ˆå¸Œï¼Œè·³è¿‡ã€‚")
                continue
            if h in global_hash_set:
                os.remove(local_path)
                print(f"   âš ï¸ é‡å¤ï¼ˆMD5ï¼‰å·²ç§»é™¤ï¼š{local_path}")
                continue
            global_hash_set.add(h)

            # å†™å…¥å…ƒæ•°æ®
            csv_writer.writerow([
                keyword,
                local_path,
                img_url,
                page_url,
                urlparse(page_url).netloc if page_url else ""
            ])

            downloaded += 1
            print(f"   âœ” {downloaded}/{max_num} ä¿å­˜ï¼š{local_path}")

        # ä¸‹ä¸€é¡µ
        page_first += len(items)
        # éšæœºå»¶è¿Ÿï¼Œé¿å…è¢«å°
        time.sleep(random.uniform(*REQUEST_DELAY_RANGE))

    print(f"âœ… å…³é”®è¯ã€{keyword}ã€‘å®Œæˆï¼Œå…± {downloaded} å¼ ã€‚")


# --------------------------
# æ€»æ§
# --------------------------
def main():
    keywords = read_keywords(CONFIG_PATH)
    if not keywords:
        print("é…ç½®æ–‡ä»¶æ— å…³é”®è¯ï¼Œé€€å‡ºã€‚")
        return

    os.makedirs(OUTPUT_ROOT, exist_ok=True)
    global_hash_set = set()

    with open(METADATA_FILE, "w", newline="", encoding="utf-8") as f:
        writer = csv.writer(f)
        writer.writerow(["keyword", "local_path", "image_url", "source_page", "source_domain"])

        for kw in keywords:
            crawl_one_keyword(kw, MAX_PER_KEYWORD, writer, global_hash_set)

    print(f"\nğŸ“„ æ‰€æœ‰å…³é”®è¯å®Œæˆï¼Œå…ƒæ•°æ®å†™å…¥ï¼š{METADATA_FILE}")


# --------------------------
# å…¥å£
# --------------------------
if __name__ == "__main__":
    main()
